# 3.9 多層感知機的從零開始實現

我們已經從上一節裡瞭解了多層感知機的原理。下面，我們一起來動手實現一個多層感知機。首先導入實現所需的包或模塊。

``` python
import torch
import numpy as np
import sys
sys.path.append("..")
import d2lzh_pytorch as d2l
```

## 3.9.1 獲取和讀取數據

這裡繼續使用Fashion-MNIST數據集。我們將使用多層感知機對圖像進行分類。

``` python
batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
```

## 3.9.2 定義模型參數

我們在3.6節（softmax迴歸的從零開始實現）裡已經介紹了，Fashion-MNIST數據集中圖像形狀為 $28 \times 28$，類別數為10。本節中我們依然使用長度為 $28 \times 28 = 784$ 的向量表示每一張圖像。因此，輸入個數為784，輸出個數為10。實驗中，我們設超參數隱藏單元個數為256。

``` python
num_inputs, num_outputs, num_hiddens = 784, 10, 256

W1 = torch.tensor(np.random.normal(0, 0.01, (num_inputs, num_hiddens)), dtype=torch.float)
b1 = torch.zeros(num_hiddens, dtype=torch.float)
W2 = torch.tensor(np.random.normal(0, 0.01, (num_hiddens, num_outputs)), dtype=torch.float)
b2 = torch.zeros(num_outputs, dtype=torch.float)

params = [W1, b1, W2, b2]
for param in params:
    param.requires_grad_(requires_grad=True)
```

## 3.9.3 定義激活函數

這裡我們使用基礎的`max`函數來實現ReLU，而非直接調用`relu`函數。

``` python
def relu(X):
    return torch.max(input=X, other=torch.tensor(0.0))
```

## 3.9.4 定義模型

同softmax迴歸一樣，我們通過`view`函數將每張原始圖像改成長度為`num_inputs`的向量。然後我們實現上一節中多層感知機的計算表達式。

``` python
def net(X):
    X = X.view((-1, num_inputs))
    H = relu(torch.matmul(X, W1) + b1)
    return torch.matmul(H, W2) + b2
```

## 3.9.5 定義損失函數

為了得到更好的數值穩定性，我們直接使用PyTorch提供的包括softmax運算和交叉熵損失計算的函數。

``` python
loss = torch.nn.CrossEntropyLoss()
```

## 3.9.6 訓練模型

訓練多層感知機的步驟和3.6節中訓練softmax迴歸的步驟沒什麼區別。我們直接調用`d2lzh_pytorch`包中的`train_ch3`函數，它的實現已經在3.6節裡介紹過。我們在這裡設超參數迭代週期數為5，學習率為100.0。
> 注：由於原書的mxnet中的`SoftmaxCrossEntropyLoss`在反向傳播的時候相對於沿batch維求和了，而PyTorch默認的是求平均，所以用PyTorch計算得到的loss比mxnet小很多（大概是maxnet計算得到的1/batch_size這個量級），所以反向傳播得到的梯度也小很多，所以為了得到差不多的學習效果，我們把學習率調得成原書的約batch_size倍，原書的學習率為0.5，這裡設置成100.0。(之所以這麼大，應該是因為d2lzh_pytorch裡面的sgd函數在更新的時候除以了batch_size，其實PyTorch在計算loss的時候已經除過一次了，sgd這裡應該不用除了)

``` python
num_epochs, lr = 5, 100.0
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params, lr)
```
輸出：
```
epoch 1, loss 0.0030, train acc 0.714, test acc 0.753
epoch 2, loss 0.0019, train acc 0.821, test acc 0.777
epoch 3, loss 0.0017, train acc 0.842, test acc 0.834
epoch 4, loss 0.0015, train acc 0.857, test acc 0.839
epoch 5, loss 0.0014, train acc 0.865, test acc 0.845
```

## 小結

* 可以通過手動定義模型及其參數來實現簡單的多層感知機。
* 當多層感知機的層數較多時，本節的實現方法會顯得較煩瑣，例如在定義模型參數的時候。

-----------
> 注：本節除了代碼之外與原書基本相同，[原書傳送門](https://zh.d2l.ai/chapter_deep-learning-basics/mlp-scratch.html)

