# 3.4 softmax迴歸

前幾節介紹的線性迴歸模型適用於輸出為連續值的情景。在另一類情景中，模型輸出可以是一個像圖像類別這樣的離散值。對於這樣的離散值預測問題，我們可以使用諸如softmax迴歸在內的分類模型。和線性迴歸不同，softmax迴歸的輸出單元從一個變成了多個，且引入了softmax運算使輸出更適合離散值的預測和訓練。本節以softmax迴歸模型為例，介紹神經網絡中的分類模型。


## 3.4.1 分類問題

讓我們考慮一個簡單的圖像分類問題，其輸入圖像的高和寬均為2像素，且色彩為灰度。這樣每個像素值都可以用一個標量表示。我們將圖像中的4像素分別記為$x_1, x_2, x_3, x_4$。假設訓練數據集中圖像的真實標籤為狗、貓或雞（假設可以用4像素表示出這3種動物），這些標籤分別對應離散值$y_1, y_2, y_3$。

我們通常使用離散的數值來表示類別，例如$y_1=1, y_2=2, y_3=3$。如此，一張圖像的標籤為1、2和3這3個數值中的一個。雖然我們仍然可以使用迴歸模型來進行建模，並將預測值就近定點化到1、2和3這3個離散值之一，但這種連續值到離散值的轉化通常會影響到分類質量。因此我們一般使用更加適合離散值輸出的模型來解決分類問題。

## 3.4.2 softmax迴歸模型

softmax迴歸跟線性迴歸一樣將輸入特徵與權重做線性疊加。與線性迴歸的一個主要不同在於，softmax迴歸的輸出值個數等於標籤裡的類別數。因為一共有4種特徵和3種輸出動物類別，所以權重包含12個標量（帶下標的$w$）、偏差包含3個標量（帶下標的$b$），且對每個輸入計算$o_1, o_2, o_3$這3個輸出：

$$
\begin{aligned}
o_1 &= x_1 w_{11} + x_2 w_{21} + x_3 w_{31} + x_4 w_{41} + b_1,\\
o_2 &= x_1 w_{12} + x_2 w_{22} + x_3 w_{32} + x_4 w_{42} + b_2,\\
o_3 &= x_1 w_{13} + x_2 w_{23} + x_3 w_{33} + x_4 w_{43} + b_3.
\end{aligned}
$$


圖3.2用神經網絡圖描繪了上面的計算。softmax迴歸同線性迴歸一樣，也是一個單層神經網絡。由於每個輸出$o_1, o_2, o_3$的計算都要依賴於所有的輸入$x_1, x_2, x_3, x_4$，softmax迴歸的輸出層也是一個全連接層。

<div align=center>
<img width="350" src="../img/chapter03/3.4_softmaxreg.svg"/>
</div>
<div align=center> 圖3.2 softmax迴歸是一個單層神經網絡</div>


既然分類問題需要得到離散的預測輸出，一個簡單的辦法是將輸出值$o_i$當作預測類別是$i$的置信度，並將值最大的輸出所對應的類作為預測輸出，即輸出 $\underset{i}{\arg\max} o_i$。例如，如果$o_1,o_2,o_3$分別為$0.1,10,0.1$，由於$o_2$最大，那麼預測類別為2，其代表貓。

然而，直接使用輸出層的輸出有兩個問題。一方面，由於輸出層的輸出值的範圍不確定，我們難以直觀上判斷這些值的意義。例如，剛才舉的例子中的輸出值10表示“很置信”圖像類別為貓，因為該輸出值是其他兩類的輸出值的100倍。但如果$o_1=o_3=10^3$，那麼輸出值10卻又表示圖像類別為貓的概率很低。另一方面，由於真實標籤是離散值，這些離散值與不確定範圍的輸出值之間的誤差難以衡量。

softmax運算符（softmax operator）解決了以上兩個問題。它通過下式將輸出值變換成值為正且和為1的概率分佈：

$$
\hat{y}_1, \hat{y}_2, \hat{y}_3 = \text{softmax}(o_1, o_2, o_3)
$$

其中

$$
\hat{y}_1 = \frac{ \exp(o_1)}{\sum_{i=1}^3 \exp(o_i)},\quad
\hat{y}_2 = \frac{ \exp(o_2)}{\sum_{i=1}^3 \exp(o_i)},\quad
\hat{y}_3 = \frac{ \exp(o_3)}{\sum_{i=1}^3 \exp(o_i)}.
$$

容易看出$\hat{y}_1 + \hat{y}_2 + \hat{y}_3 = 1$且$0 \leq \hat{y}_1, \hat{y}_2, \hat{y}_3 \leq 1$，因此$\hat{y}_1, \hat{y}_2, \hat{y}_3$是一個合法的概率分佈。這時候，如果$\hat{y}_2=0.8$，不管$\hat{y}_1$和$\hat{y}_3$的值是多少，我們都知道圖像類別為貓的概率是80%。此外，我們注意到

$$
\underset{i}{\arg\max} o_i = \underset{i}{\arg\max} \hat{y}_i
$$

因此softmax運算不改變預測類別輸出。

## 3.4.3 單樣本分類的矢量計算表達式

為了提高計算效率，我們可以將單樣本分類通過矢量計算來表達。在上面的圖像分類問題中，假設softmax迴歸的權重和偏差參數分別為

$$
\boldsymbol{W} = 
\begin{bmatrix}
    w_{11} & w_{12} & w_{13} \\
    w_{21} & w_{22} & w_{23} \\
    w_{31} & w_{32} & w_{33} \\
    w_{41} & w_{42} & w_{43}
\end{bmatrix},\quad
\boldsymbol{b} = 
\begin{bmatrix}
    b_1 & b_2 & b_3
\end{bmatrix},
$$

設高和寬分別為2個像素的圖像樣本$i$的特徵為

$$\boldsymbol{x}^{(i)} = \begin{bmatrix}x_1^{(i)} & x_2^{(i)} & x_3^{(i)} & x_4^{(i)}\end{bmatrix},$$

輸出層的輸出為

$$\boldsymbol{o}^{(i)} = \begin{bmatrix}o_1^{(i)} & o_2^{(i)} & o_3^{(i)}\end{bmatrix},$$

預測為狗、貓或雞的概率分佈為

$$\boldsymbol{\hat{y}}^{(i)} = \begin{bmatrix}\hat{y}_1^{(i)} & \hat{y}_2^{(i)} & \hat{y}_3^{(i)}\end{bmatrix}.$$


softmax迴歸對樣本$i$分類的矢量計算表達式為

$$
\begin{aligned}
\boldsymbol{o}^{(i)} &= \boldsymbol{x}^{(i)} \boldsymbol{W} + \boldsymbol{b},\\
\boldsymbol{\hat{y}}^{(i)} &= \text{softmax}(\boldsymbol{o}^{(i)}).
\end{aligned}
$$

## 3.4.4 小批量樣本分類的矢量計算表達式


為了進一步提升計算效率，我們通常對小批量數據做矢量計算。廣義上講，給定一個小批量樣本，其批量大小為$n$，輸入個數（特徵數）為$d$，輸出個數（類別數）為$q$。設批量特徵為$\boldsymbol{X} \in \mathbb{R}^{n \times d}$。假設softmax迴歸的權重和偏差參數分別為$\boldsymbol{W} \in \mathbb{R}^{d \times q}$和$\boldsymbol{b} \in \mathbb{R}^{1 \times q}$。softmax迴歸的矢量計算表達式為

$$
\begin{aligned}
\boldsymbol{O} &= \boldsymbol{X} \boldsymbol{W} + \boldsymbol{b},\\
\boldsymbol{\hat{Y}} &= \text{softmax}(\boldsymbol{O}),
\end{aligned}
$$

其中的加法運算使用了廣播機制，$\boldsymbol{O}, \boldsymbol{\hat{Y}} \in \mathbb{R}^{n \times q}$且這兩個矩陣的第$i$行分別為樣本$i$的輸出$\boldsymbol{o}^{(i)}$和概率分佈$\boldsymbol{\hat{y}}^{(i)}$。


## 3.4.5 交叉熵損失函數

前面提到，使用softmax運算後可以更方便地與離散標籤計算誤差。我們已經知道，softmax運算將輸出變換成一個合法的類別預測分佈。實際上，真實標籤也可以用類別分佈表達：對於樣本$i$，我們構造向量$\boldsymbol{y}^{(i)}\in \mathbb{R}^{q}$ ，使其第$y^{(i)}$（樣本$i$類別的離散數值）個元素為1，其餘為0。這樣我們的訓練目標可以設為使預測概率分佈$\boldsymbol{\hat y}^{(i)}$儘可能接近真實的標籤概率分佈$\boldsymbol{y}^{(i)}$。

我們可以像線性迴歸那樣使用平方損失函數$\|\boldsymbol{\hat y}^{(i)}-\boldsymbol{y}^{(i)}\|^2/2$。然而，想要預測分類結果正確，我們其實並不需要預測概率完全等於標籤概率。例如，在圖像分類的例子裡，如果$y^{(i)}=3$，那麼我們只需要$\hat{y}^{(i)}_3$比其他兩個預測值$\hat{y}^{(i)}_1$和$\hat{y}^{(i)}_2$大就行了。即使$\hat{y}^{(i)}_3$值為0.6，不管其他兩個預測值為多少，類別預測均正確。而平方損失則過於嚴格，例如$\hat y^{(i)}_1=\hat y^{(i)}_2=0.2$比$\hat y^{(i)}_1=0, \hat y^{(i)}_2=0.4$的損失要小很多，雖然兩者都有同樣正確的分類預測結果。

改善上述問題的一個方法是使用更適合衡量兩個概率分佈差異的測量函數。其中，交叉熵（cross entropy）是一個常用的衡量方法：

$$H\left(\boldsymbol y^{(i)}, \boldsymbol {\hat y}^{(i)}\right ) = -\sum_{j=1}^q y_j^{(i)} \log \hat y_j^{(i)},$$

其中帶下標的$y_j^{(i)}$是向量$\boldsymbol y^{(i)}$中非0即1的元素，需要注意將它與樣本$i$類別的離散數值，即不帶下標的$y^{(i)}$區分。在上式中，我們知道向量$\boldsymbol y^{(i)}$中只有第$y^{(i)}$個元素$y^{(i)}_{y^{(i)}}$為1，其餘全為0，於是$H(\boldsymbol y^{(i)}, \boldsymbol {\hat y}^{(i)}) = -\log \hat y_{y^{(i)}}^{(i)}$。也就是說，交叉熵只關心對正確類別的預測概率，因為只要其值足夠大，就可以確保分類結果正確。當然，遇到一個樣本有多個標籤時，例如圖像裡含有不止一個物體時，我們並不能做這一步簡化。但即便對於這種情況，交叉熵同樣只關心對圖像中出現的物體類別的預測概率。


假設訓練數據集的樣本數為$n$，交叉熵損失函數定義為
$$\ell(\boldsymbol{\Theta}) = \frac{1}{n} \sum_{i=1}^n H\left(\boldsymbol y^{(i)}, \boldsymbol {\hat y}^{(i)}\right ),$$

其中$\boldsymbol{\Theta}$代表模型參數。同樣地，如果每個樣本只有一個標籤，那麼交叉熵損失可以簡寫成$\ell(\boldsymbol{\Theta}) = -(1/n)  \sum_{i=1}^n \log \hat y_{y^{(i)}}^{(i)}$。從另一個角度來看，我們知道最小化$\ell(\boldsymbol{\Theta})$等價於最大化$\exp(-n\ell(\boldsymbol{\Theta}))=\prod_{i=1}^n \hat y_{y^{(i)}}^{(i)}$，即最小化交叉熵損失函數等價於最大化訓練數據集所有標籤類別的聯合預測概率。


## 3.4.6 模型預測及評價

在訓練好softmax迴歸模型後，給定任一樣本特徵，就可以預測每個輸出類別的概率。通常，我們把預測概率最大的類別作為輸出類別。如果它與真實類別（標籤）一致，說明這次預測是正確的。在3.6節的實驗中，我們將使用準確率（accuracy）來評價模型的表現。它等於正確預測數量與總預測數量之比。

## 小結

* softmax迴歸適用於分類問題。它使用softmax運算輸出類別的概率分佈。
* softmax迴歸是一個單層神經網絡，輸出個數等於分類問題中的類別個數。
* 交叉熵適合衡量兩個概率分佈的差異。

-----------
> 注：本節與原書基本相同，[原書此節傳送門](https://zh.d2l.ai/chapter_deep-learning-basics/softmax-regression.html)
