# 深度學習簡介
你可能已經接觸過編程，並開發過一兩款程序。同時你可能讀過關於深度學習或者機器學習的鋪天蓋地的報道，儘管很多時候它們被賦予了更廣義的名字：人工智能。實際上，或者說幸運的是，大部分程序並不需要深度學習或者是更廣義上的人工智能技術。例如，如果我們要為一臺微波爐編寫一個用戶界面，只需要一點兒工夫我們便能設計出十幾個按鈕以及一系列能精確描述微波爐在各種情況下的表現的規則。再比如，假設我們要編寫一個電子郵件客戶端。這樣的程序比微波爐要複雜一些，但我們還是可以沉下心來一步一步思考：客戶端的用戶界面將需要幾個輸入框來接受收件人、主題、郵件正文等，程序將監聽鍵盤輸入並寫入一個緩衝區，然後將它們顯示在相應的輸入框中。當用戶點擊“發送”按鈕時，我們需要檢查收件人郵箱地址的格式是否正確，並檢查郵件主題是否為空，或在主題為空時警告用戶，而後用相應的協議傳送郵件。

值得注意的是，在以上兩個例子中，我們都不需要收集真實世界中的數據，也不需要系統地提取這些數據的特徵。只要有充足的時間，我們的常識與編程技巧已經足夠讓我們完成任務。

與此同時，我們很容易就能找到一些連世界上最好的程序員也無法僅用編程技巧解決的簡單問題。例如，假設我們想要編寫一個判定一張圖像中有沒有貓的程序。這件事聽起來好像很簡單，對不對？程序只需要對每張輸入圖像輸出“真”（表示有貓）或者“假”（表示無貓）即可。但令人驚訝的是，即使是世界上最優秀的計算機科學家和程序員也不懂如何編寫這樣的程序。

我們該從哪裡入手呢？我們先進一步簡化這個問題：若假設所有圖像的高和寬都是同樣的400像素大小，一個像素由紅綠藍三個值構成，那麼一張圖像就由近50萬個數值表示。那麼哪些數值隱藏著我們需要的信息呢？是所有數值的平均數，還是四個角的數值，抑或是圖像中的某一個特別的點？事實上，要想解讀圖像中的內容，需要尋找僅僅在結合成千上萬的數值時才會出現的特徵，如邊緣、質地、形狀、眼睛、鼻子等，最終才能判斷圖像中是否有貓。

一種解決以上問題的思路是逆向思考。與其設計一個解決問題的程序，不如從最終的需求入手來尋找一個解決方案。事實上，這也是目前的機器學習和深度學習應用共同的核心思想：我們可以稱其為“用數據編程”。與其枯坐在房間裡思考怎麼設計一個識別貓的程序，不如利用人類肉眼在圖像中識別貓的能力。我們可以收集一些已知包含貓與不包含貓的真實圖像，然後我們的目標就轉化成如何從這些圖像入手得到一個可以推斷出圖像中是否有貓的函數。這個函數的形式通常通過我們的知識來針對特定問題選定。例如，我們使用一個二次函數來判斷圖像中是否有貓，但是像二次函數係數值這樣的函數參數的具體值則是通過數據來確定。

通俗來說，機器學習是一門討論各式各樣的適用於不同問題的函數形式，以及如何使用數據來有效地獲取函數參數具體值的學科。深度學習是指機器學習中的一類函數，它們的形式通常為多層神經網絡。近年來，仰仗著大數據集和強大的硬件，深度學習已逐漸成為處理圖像、文本語料和聲音信號等複雜高維度數據的主要方法。

我們現在正處於一個程序設計得到深度學習的幫助越來越多的時代。這可以說是計算機科學歷史上的一個分水嶺。舉個例子，深度學習已經在你的手機裡：拼寫校正、語音識別、認出社交媒體照片裡的好友們等。得益於優秀的算法、快速而廉價的算力、前所未有的大量數據以及強大的軟件工具，如今大多數軟件工程師都有能力建立複雜的模型來解決十年前連最優秀的科學家都覺得棘手的問題。

本書希望能幫助讀者進入深度學習的浪潮中。我們希望結合數學、代碼和樣例讓深度學習變得觸手可及。本書不要求具有高深的數學或編程背景，我們將隨著章節的發展逐一解釋所需要的知識。更值得一提的是，本書的每一節都是一個可以獨立運行的Jupyter記事本。讀者可以從網上獲得這些記事本，並且可以在個人電腦或雲端服務器上執行它們。這樣讀者就可以隨意改動書中的代碼並得到及時反饋。我們希望本書能幫助和啟發新一代的程序員、創業者、統計學家、生物學家，以及所有對深度學習感興趣的人。


## 起源

雖然深度學習似乎是最近幾年剛興起的名詞，但它所基於的神經網絡模型和用數據編程的核心思想已經被研究了數百年。自古以來，人類就一直渴望能從數據中分析出預知未來的竅門。實際上，數據分析正是大部分自然科學的本質，我們希望從日常的觀測中提取規則，並找尋不確定性。

早在17世紀，[雅各比·伯努利（1655--1705）](https://en.wikipedia.org/wiki/Jacob_Bernoulli)提出了描述只有兩種結果的隨機過程（如拋擲一枚硬幣）的伯努利分佈。大約一個世紀之後，[卡爾·弗里德里希·高斯（1777--1855）](https://en.wikipedia.org/wiki/Carl_Friedrich_Gauss)發明了今日仍廣泛用在從保險計算到醫學診斷等領域的最小二乘法。概率論、統計學和模式識別等工具幫助自然科學的實驗學家們從數據迴歸到自然定律，從而發現瞭如歐姆定律（描述電阻兩端電壓和流經電阻電流關係的定律）這類可以用線性模型完美表達的一系列自然法則。

即使是在中世紀，數學家也熱衷於利用統計學來做出估計。例如，在[雅各比·科貝爾（1460--1533）](https://www.maa.org/press/periodicals/convergence/mathematical-treasures-jacob-kobels-geometry)的幾何書中記載了使用16名男子的平均腳長來估計男子的平均腳長。

<div align=center>
<img width="600" src="../img/chapter01/1.1_koebel.jpg"/>
</div>
<center>圖1.1 在中世紀，16名男子的平均腳長被用來估計男子的平均腳長</center>



如圖1.1所示，在這個研究中，16位成年男子被要求在離開教堂時站成一排並把腳貼在一起，而後他們腳的總長度除以16得到了一個估計：這個數字大約相當於今日的一英尺。這個算法之後又被改進，以應對特異形狀的腳：最長和最短的腳不計入，只對剩餘的腳長取平均值，即裁剪平均值的雛形。

現代統計學在20世紀的真正起飛要歸功於數據的收集和發佈。統計學巨匠之一[羅納德·費雪（1890--1962）](https://en.wikipedia.org/wiki/Ronald_Fisher)對統計學理論和統計學在基因學中的應用功不可沒。他發明的許多算法和公式，例如線性判別分析和費雪信息，仍經常被使用。即使是他在1936年發佈的Iris數據集，仍然偶爾被用於演示機器學習算法。

[克勞德·香農（1916--2001）](https://en.wikipedia.org/wiki/Claude_Shannon)的信息論以及[阿蘭·圖靈 （1912--1954）](https://en.wikipedia.org/wiki/Allan_Turing)的計算理論也對機器學習有深遠影響。圖靈在他著名的論文[《計算機器與智能》](https://www.jstor.org/stable/2251299)中提出了“機器可以思考嗎？”這樣一個問題 [1]。在他描述的“圖靈測試”中，如果一個人在使用文本交互時不能區分他的對話對象到底是人類還是機器的話，那麼即可認為這臺機器是有智能的。時至今日，智能機器的發展可謂日新月異。

另一個對深度學習有重大影響的領域是神經科學與心理學。既然人類顯然能夠展現出智能，那麼對於解釋並逆向工程人類智能機理的探究也在情理之中。最早的算法之一是由[唐納德·赫布（1904--1985）](https://en.wikipedia.org/wiki/Donald_O._Hebb)正式提出的。在他開創性的著作[《行為的組織》](http://s-f-walker.org.uk/pubsebooks/pdfs/The_Organization_of_Behavior-Donald_O._Hebb.pdf)中，他提出神經是通過正向強化來學習的，即赫布理論 [2]。赫布理論是感知機學習算法的原型，併成為支撐今日深度學習的隨機梯度下降算法的基石：強化合意的行為、懲罰不合意的行為，最終獲得優良的神經網絡參數。

來源於生物學的靈感是神經網絡名字的由來。這類研究者可以追溯到一個多世紀前的[亞歷山大·貝恩（1818--1903）](https://en.wikipedia.org/wiki/Alexander_Bain)和[查爾斯·斯科特·謝靈頓（1857--1952）](https://en.wikipedia.org/wiki/Charles_Scott_Sherrington)。研究者們嘗試組建模仿神經元互動的計算電路。隨著時間發展，神經網絡的生物學解釋被稀釋，但仍保留了這個名字。時至今日，絕大多數神經網絡都包含以下的核心原則。

* 交替使用線性處理單元與非線性處理單元，它們經常被稱為“層”。
* 使用鏈式法則（即反向傳播）來更新網絡的參數。

在最初的快速發展之後，自約1995年起至2005年，大部分機器學習研究者的視線從神經網絡上移開了。這是由於多種原因。首先，訓練神經網絡需要極強的計算力。儘管20世紀末內存已經足夠，計算力卻不夠充足。其次，當時使用的數據集也相對小得多。費雪在1936年發佈的的Iris數據集僅有150個樣本，並被廣泛用於測試算法的性能。具有6萬個樣本的MNIST數據集在當時已經被認為是非常龐大了，儘管它如今已被認為是典型的簡單數據集。由於數據和計算力的稀缺，從經驗上來說，如核方法、決策樹和概率圖模型等統計工具更優。它們不像神經網絡一樣需要長時間的訓練，並且在強大的理論保證下提供可以預測的結果。

## 發展

互聯網的崛起、價廉物美的傳感器和低價的存儲器令我們越來越容易獲取大量數據。加之便宜的計算力，尤其是原本為電腦遊戲設計的GPU的出現，上文描述的情況改變了許多。一瞬間，原本被認為不可能的算法和模型變得觸手可及。這樣的發展趨勢從如下表格中可見一斑。

|年代|數據樣本個數|內存|每秒浮點計算數|
|:--|:-:|:-:|:-:|
|1970|100（Iris）|1 KB|100 K（Intel 8080）|
|1980|1 K（波士頓房價）|100 KB|1 M（Intel 80186）|
|1990|10 K（手寫字符識別）|10 MB|10 M（Intel 80486）|
|2000|10 M（網頁）|100 MB|1 G（Intel Core）|
|2010|10 G（廣告）|1 GB|1 T（NVIDIA C2050）|
|2020|1 T（社交網絡）|100 GB|1 P（NVIDIA DGX-2）|

很顯然，存儲容量沒能跟上數據量增長的步伐。與此同時，計算力的增長又蓋過了數據量的增長。這樣的趨勢使得統計模型可以在優化參數上投入更多的計算力，但同時需要提高存儲的利用效率，例如使用非線性處理單元。這也相應導致了機器學習和統計學的最優選擇從廣義線性模型及核方法變化為深度多層神經網絡。這樣的變化正是諸如多層感知機、卷積神經網絡、長短期記憶循環神經網絡和Q學習等深度學習的支柱模型在過去10年從坐了數十年的冷板凳上站起來被“重新發現”的原因。

近年來在統計模型、應用和算法上的進展常被拿來與寒武紀大爆發（歷史上物種數量大爆發的一個時期）做比較。但這些進展不僅僅是因為可用資源變多了而讓我們得以用新瓶裝舊酒。下面的列表僅僅涵蓋了近十年來深度學習長足發展的部分原因。

* 優秀的容量控制方法，如丟棄法，使大型網絡的訓練不再受制於過擬合（大型神經網絡學會記憶大部分訓練數據的行為） [3]。這是靠在整個網絡中注入噪聲而達到的，如訓練時隨機將權重替換為隨機的數字 [4]。

* 注意力機制解決了另一個困擾統計學超過一個世紀的問題：如何在不增加參數的情況下擴展一個系統的記憶容量和複雜度。注意力機制使用了一個可學習的指針結構來構建出一個精妙的解決方法 [5]。也就是說，與其在像機器翻譯這樣的任務中記憶整個句子，不如記憶指向翻譯的中間狀態的指針。由於生成譯文前不需要再存儲整句原文的信息，這樣的結構使準確翻譯長句變得可能。

* 記憶網絡 [6]和神經編碼器—解釋器 [7]這樣的多階設計使得針對推理過程的迭代建模方法變得可能。這些模型允許重複修改深度網絡的內部狀態，這樣就能模擬出推理鏈條上的各個步驟，就好像處理器在計算過程中修改內存一樣。

* 另一個重大發展是生成對抗網絡的發明 [8]。傳統上，用在概率分佈估計和生成模型上的統計方法更多地關注於找尋正確的概率分佈，以及正確的採樣算法。生成對抗網絡的關鍵創新在於將採樣部分替換成了任意的含有可微分參數的算法。這些參數將被訓練到使辨別器不能再分辨真實的和生成的樣本。生成對抗網絡可使用任意算法來生成輸出的這一特性為許多技巧打開了新的大門。例如生成奔跑的斑馬 [9]和生成名流的照片 [10] 都是生成對抗網絡發展的見證。

* 許多情況下單個GPU已經不能滿足在大型數據集上進行訓練的需要。過去10年內我們構建分佈式並行訓練算法的能力已經有了極大的提升。設計可擴展算法的最大瓶頸在於深度學習優化算法的核心：隨機梯度下降需要相對更小的批量。與此同時，更小的批量也會降低GPU的效率。如果使用1,024個GPU，每個GPU的批量大小為32個樣本，那麼單步訓練的批量大小將是32,000個以上。近年來李沐 [11]、Yang You等人 [12]以及Xianyan Jia等人 [13]的工作將批量大小增至多達64,000個樣例，並把在ImageNet數據集上訓練ResNet-50模型的時間降到了7分鐘。與之對比，最初的訓練時間需要以天來計算。

* 並行計算的能力也為至少在可以採用模擬情況下的強化學習的發展貢獻了力量。並行計算幫助計算機在圍棋、雅達利遊戲、星際爭霸和物理模擬上達到了超過人類的水準。

* 深度學習框架也在傳播深度學習思想的過程中扮演了重要角色。[Caffe](https://github.com/BVLC/caffe)、 [Torch](https://github.com/torch)和[Theano](https://github.com/Theano/Theano)這樣的第一代框架使建模變得更簡單。許多開創性的論文都用到了這些框架。如今它們已經被[TensorFlow](https://github.com/tensorflow/tensorflow)（經常是以高層API [Keras](https://github.com/keras-team/keras)的形式被使用）、[CNTK](https://github.com/Microsoft/CNTK)、 [Caffe 2](https://github.com/caffe2/caffe2) 和[Apache MXNet](https://github.com/apache/incubator-mxnet)所取代。第三代，即命令式深度學習框架，是由用類似NumPy的語法來定義模型的 [Chainer](https://github.com/chainer/chainer)所開創的。這樣的思想後來被 [PyTorch](https://github.com/pytorch/pytorch)和MXNet的[Gluon API](https://github.com/apache/incubator-mxnet) 採用，後者也正是本書用來教學深度學習的工具。

系統研究者負責構建更好的工具，統計學家建立更好的模型。這樣的分工使工作大大簡化。舉例來說，在2014年時，訓練一個邏輯迴歸模型曾是卡內基梅隆大學佈置給機器學習方向的新入學博士生的作業問題。時至今日，這個問題只需要少於10行的代碼便可以完成，普通的程序員都可以做到。

## 成功案例

長期以來機器學習總能完成其他方法難以完成的目標。例如，自20世紀90年代起，郵件的分揀就開始使用光學字符識別。實際上這正是知名的MNIST和USPS手寫數字數據集的來源。機器學習也是電子支付系統的支柱，可以用於讀取銀行支票、進行授信評分以及防止金融欺詐。機器學習算法在網絡上被用來提供搜索結果、個性化推薦和網頁排序。雖然長期處於公眾視野之外，但是機器學習已經滲透到了我們工作和生活的方方面面。直到近年來，在此前認為無法被解決的問題以及直接關係到消費者的問題上取得突破性進展後，機器學習才逐漸變成公眾的焦點。這些進展基本歸功於深度學習。

* 蘋果公司的Siri、亞馬遜的Alexa和谷歌助手一類的智能助手能以可觀的準確率回答口頭提出的問題，甚至包括從簡單的開關燈具（對殘疾群體幫助很大）到提供語音對話幫助。智能助手的出現或許可以作為人工智能開始影響我們生活的標誌。

* 智能助手的關鍵是需要能夠精確識別語音，而這類系統在某些應用上的精確度已經漸漸增長到可以與人類比肩 [14]。

* 物體識別也經歷了漫長的發展過程。在2010年從圖像中識別出物體的類別仍是一個相當有挑戰性的任務。當年日本電氣、伊利諾伊大學香檳分校和羅格斯大學團隊在ImageNet基準測試上取得了28%的前五錯誤率 [15]。到2017年，這個數字降低到了2.25% [16]。研究人員在鳥類識別和皮膚癌診斷上，也取得了同樣驚世駭俗的成績。

* 遊戲曾被認為是人類智能最後的堡壘。自使用時間差分強化學習玩雙陸棋的TD-Gammon開始，算法和算力的發展催生了一系列在遊戲上使用的新算法。與雙陸棋不同，國際象棋有更復雜的狀態空間和更多的可選動作。“深藍”用大量的並行、專用硬件和遊戲樹的高效搜索打敗了加里·卡斯帕羅夫 [17]。圍棋因其龐大的狀態空間被認為是更難的遊戲，AlphaGo在2016年用結合深度學習與蒙特卡洛樹採樣的方法達到了人類水準 [18]。對德州撲克遊戲而言，除了巨大的狀態空間之外，更大的挑戰是遊戲的信息並不完全可見，例如看不到對手的牌。而“冷撲大師”用高效的策略體系超越了人類玩家的表現 [19]。以上的例子都體現出了先進的算法是人工智能在遊戲上的表現提升的重要原因。

* 機器學習進步的另一個標誌是自動駕駛汽車的發展。儘管距離完全的自主駕駛還有很長的路要走，但諸如[Tesla](http://www.tesla.com)、[NVIDIA](http://www.nvidia.com)、 [MobilEye](http://www.mobileye.com)和[Waymo](http://www.waymo.com)這樣的公司發佈的具有部分自主駕駛功能的產品展示出了這個領域巨大的進步。完全自主駕駛的難點在於它需要將感知、思考和規則整合在同一個系統中。目前，深度學習主要被應用在計算機視覺的部分，剩餘的部分還是需要工程師們的大量調試。

以上列出的僅僅是近年來深度學習所取得的成果的冰山一角。機器人學、物流管理、計算生物學、粒子物理學和天文學近年來的發展也有一部分要歸功於深度學習。可以看到，深度學習已經逐漸演變成一個工程師和科學家皆可使用的普適工具。


## 特點

在描述深度學習的特點之前，我們先回顧並概括一下機器學習和深度學習的關係。機器學習研究如何使計算機系統利用經驗改善性能。它是人工智能領域的分支，也是實現人工智能的一種手段。在機器學習的眾多研究方向中，表徵學習關注如何自動找出表示數據的合適方式，以便更好地將輸入變換為正確的輸出，而本書要重點探討的深度學習是具有多級表示的表徵學習方法。在每一級（從原始數據開始），深度學習通過簡單的函數將該級的表示變換為更高級的表示。因此，深度學習模型也可以看作是由許多簡單函數複合而成的函數。當這些複合的函數足夠多時，深度學習模型就可以表達非常複雜的變換。

深度學習可以逐級表示越來越抽象的概念或模式。以圖像為例，它的輸入是一堆原始像素值。深度學習模型中，圖像可以逐級表示為特定位置和角度的邊緣、由邊緣組合得出的花紋、由多種花紋進一步匯合得到的特定部位的模式等。最終，模型能夠較容易根據更高級的表示完成給定的任務，如識別圖像中的物體。值得一提的是，作為表徵學習的一種，深度學習將自動找出每一級表示數據的合適方式。

因此，深度學習的一個外在特點是端到端的訓練。也就是說，並不是將單獨調試的部分拼湊起來組成一個系統，而是將整個系統組建好之後一起訓練。比如說，計算機視覺科學家之前曾一度將特徵抽取與機器學習模型的構建分開處理，像是Canny邊緣探測 [20] 和SIFT特徵提取 [21] 曾佔據統治性地位達10年以上，但這也就是人類能找到的最好方法了。當深度學習進入這個領域後，這些特徵提取方法就被性能更強的自動優化的逐級過濾器替代了。

相似地，在自然語言處理領域，詞袋模型多年來都被認為是不二之選 [22]。詞袋模型是將一個句子映射到一個詞頻向量的模型，但這樣的做法完全忽視了單詞的排列順序或者句中的標點符號。不幸的是，我們也沒有能力來手工抽取更好的特徵。但是自動化的算法反而可以從所有可能的特徵中搜尋最好的那個，這也帶來了極大的進步。例如，語義相關的詞嵌入能夠在向量空間中完成如下推理：“柏林 - 德國 + 中國 = 北京”。可以看出，這些都是端到端訓練整個系統帶來的效果。

除端到端的訓練以外，我們也正在經歷從含參數統計模型轉向完全無參數的模型。當數據非常稀缺時，我們需要通過簡化對現實的假設來得到實用的模型。當數據充足時，我們就可以用能更好地擬合現實的無參數模型來替代這些含參數模型。這也使我們可以得到更精確的模型，儘管需要犧牲一些可解釋性。

相對其它經典的機器學習方法而言，深度學習的不同在於：對非最優解的包容、對非凸非線性優化的使用，以及勇於嘗試沒有被證明過的方法。這種在處理統計問題上的新經驗主義吸引了大量人才的湧入，使得大量實際問題有了更好的解決方案。儘管大部分情況下需要為深度學習修改甚至重新發明已經存在數十年的工具，但是這絕對是一件非常有意義並令人興奮的事。

最後，深度學習社區長期以來以在學術界和企業之間分享工具而自豪，並開源了許多優秀的軟件庫、統計模型和預訓練網絡。正是本著開放開源的精神，本書的內容和基於它的教學視頻可以自由下載和隨意分享。我們致力於為所有人降低學習深度學習的門檻，並希望大家從中獲益。


## 小結

* 機器學習研究如何使計算機系統利用經驗改善性能。它是人工智能領域的分支，也是實現人工智能的一種手段。
* 作為機器學習的一類，表徵學習關注如何自動找出表示數據的合適方式。
* 深度學習是具有多級表示的表徵學習方法。它可以逐級表示越來越抽象的概念或模式。
* 深度學習所基於的神經網絡模型和用數據編程的核心思想實際上已經被研究了數百年。
* 深度學習已經逐漸演變成一個工程師和科學家皆可使用的普適工具。


## 練習

* 你現在正在編寫的代碼有沒有可以被“學習”的部分，也就是說，是否有可以被機器學習改進的部分？
* 你在生活中有沒有這樣的場景：雖有許多展示如何解決問題的樣例，但缺少自動解決問題的算法？它們也許是深度學習的最好獵物。
* 如果把人工智能的發展看作是新一次工業革命，那麼深度學習和數據的關係是否像是蒸汽機與煤炭的關係呢？為什麼？
* 端到端的訓練方法還可以用在哪裡？物理學，工程學還是經濟學？
* 為什麼應該讓深度網絡模仿人腦結構？為什麼不該讓深度網絡模仿人腦結構？



## 參考文獻

[1] Machinery, C. (1950). Computing machinery and intelligence-AM Turing. Mind, 59(236), 433.

[2] Hebb, D. O. (1949). The organization of behavior; a neuropsycholocigal theory. A Wiley Book in Clinical Psychology., 62-78.

[3] Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014). Dropout: a simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1), 1929-1958.

[4] Bishop, C. M. (1995). Training with noise is equivalent to Tikhonov regularization. Neural computation, 7(1), 108-116.

[5] Bahdanau, D., Cho, K., & Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473.

[6] Sukhbaatar, S., Weston, J., & Fergus, R. (2015). End-to-end memory networks. In Advances in neural information processing systems (pp. 2440-2448).

[7] Reed, S., & De Freitas, N. (2015). Neural programmer-interpreters. arXiv preprint arXiv:1511.06279.

[8] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., … & Bengio, Y. (2014). Generative adversarial nets. In Advances in neural information processing systems (pp. 2672-2680).

[9] Zhu, J. Y., Park, T., Isola, P., & Efros, A. A. (2017). Unpaired image-to-image translation using cycle-consistent adversarial networks. arXiv preprint.

[10] Karras, T., Aila, T., Laine, S., & Lehtinen, J. (2017). Progressive growing of gans for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196.

[11] Li, M. (2017). Scaling Distributed Machine Learning with System and Algorithm Co-design (Doctoral dissertation, PhD thesis, Intel).

[12] You, Y., Gitman, I., & Ginsburg, B. Large batch training of convolutional networks. ArXiv e-prints.

[13] Jia, X., Song, S., He, W., Wang, Y., Rong, H., Zhou, F., … & Chen, T. (2018). Highly Scalable Deep Learning Training System with Mixed-Precision: Training ImageNet in Four Minutes. arXiv preprint arXiv:1807.11205.

[14] Xiong, W., Droppo, J., Huang, X., Seide, F., Seltzer, M., Stolcke, A., … & Zweig, G. (2017, March). The Microsoft 2016 conversational speech recognition system. In Acoustics, Speech and Signal Processing (ICASSP), 2017 IEEE International Conference on (pp. 5255-5259). IEEE.

[15] Lin, Y., Lv, F., Zhu, S., Yang, M., Cour, T., Yu, K., … & Huang, T. (2010). Imagenet classification: fast descriptor coding and large-scale svm training. Large scale visual recognition challenge.

[16] Hu, J., Shen, L., & Sun, G. (2017). Squeeze-and-excitation networks. arXiv preprint arXiv:1709.01507, 7.

[17] Campbell, M., Hoane Jr, A. J., & Hsu, F. H. (2002). Deep blue. Artificial intelligence, 134(1-2), 57-83.

[18] Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., … & Dieleman, S. (2016). Mastering the game of Go with deep neural networks and tree search. nature, 529(7587), 484.

[19] Brown, N., & Sandholm, T. (2017, August). Libratus: The superhuman ai for no-limit poker. In Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence.

[20] Canny, J. (1986). A computational approach to edge detection. IEEE Transactions on pattern analysis and machine intelligence, (6), 679-698.

[21] Lowe, D. G. (2004). Distinctive image features from scale-invariant keypoints. International journal of computer vision, 60(2), 91-110.

[22] Salton, G., & McGill, M. J. (1986). Introduction to modern information retrieval.


-----------
> 注：本節與原書基本相同，為了完整性而搬運過來，[原書傳送門](https://zh.d2l.ai/chapter_introduction/deep-learning-intro.html)
