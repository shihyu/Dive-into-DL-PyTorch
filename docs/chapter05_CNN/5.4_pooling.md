# 5.4 池化層

回憶一下，在5.1節（二維卷積層）裡介紹的圖像物體邊緣檢測應用中，我們構造卷積核從而精確地找到了像素變化的位置。設任意二維數組`X`的`i`行`j`列的元素為`X[i, j]`。如果我們構造的卷積核輸出`Y[i, j]=1`，那麼說明輸入中`X[i, j]`和`X[i, j+1]`數值不一樣。這可能意味著物體邊緣通過這兩個元素之間。但實際圖像裡，我們感興趣的物體不會總出現在固定位置：即使我們連續拍攝同一個物體也極有可能出現像素位置上的偏移。這會導致同一個邊緣對應的輸出可能出現在卷積輸出`Y`中的不同位置，進而對後面的模式識別造成不便。

在本節中我們介紹池化（pooling）層，它的提出是**為了緩解卷積層對位置的過度敏感性**。

## 5.4.1 二維最大池化層和平均池化層

同卷積層一樣，池化層每次對輸入數據的一個固定形狀窗口（又稱池化窗口）中的元素計算輸出。不同於卷積層裡計算輸入和核的互相關性，池化層直接計算池化窗口內元素的最大值或者平均值。該運算也分別叫做最大池化或平均池化。在二維最大池化中，池化窗口從輸入數組的最左上方開始，按從左往右、從上往下的順序，依次在輸入數組上滑動。當池化窗口滑動到某一位置時，窗口中的輸入子數組的最大值即輸出數組中相應位置的元素。

<div align=center>
<img width="300" src="../img/chapter05/5.4_pooling.svg"/>
</div>
<div align=center>圖5.6 池化窗口形狀為 2 x 2 的最大池化</div>

圖5.6展示了池化窗口形狀為$2\times 2$的最大池化，陰影部分為第一個輸出元素及其計算所使用的輸入元素。輸出數組的高和寬分別為2，其中的4個元素由取最大值運算$\text{max}$得出：

$$
\max(0,1,3,4)=4,\\
\max(1,2,4,5)=5,\\
\max(3,4,6,7)=7,\\
\max(4,5,7,8)=8.\\
$$


二維平均池化的工作原理與二維最大池化類似，但將最大運算符替換成平均運算符。池化窗口形狀為$p \times q$的池化層稱為$p \times q$池化層，其中的池化運算叫作$p \times q$池化。

讓我們再次回到本節開始提到的物體邊緣檢測的例子。現在我們將卷積層的輸出作為$2\times 2$最大池化的輸入。設該卷積層輸入是`X`、池化層輸出為`Y`。無論是`X[i, j]`和`X[i, j+1]`值不同，還是`X[i, j+1]`和`X[i, j+2]`不同，池化層輸出均有`Y[i, j]=1`。也就是說，使用$2\times 2$最大池化層時，只要卷積層識別的模式在高和寬上移動不超過一個元素，我們依然可以將它檢測出來。

下面把池化層的前向計算實現在`pool2d`函數裡。它跟5.1節（二維卷積層）裡`corr2d`函數非常類似，唯一的區別在計算輸出`Y`上。

``` python
import torch
from torch import nn

def pool2d(X, pool_size, mode='max'):
    X = X.float()
    p_h, p_w = pool_size
    Y = torch.zeros(X.shape[0] - p_h + 1, X.shape[1] - p_w + 1)
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            if mode == 'max':
                Y[i, j] = X[i: i + p_h, j: j + p_w].max()
            elif mode == 'avg':
                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()       
    return Y
```

我們可以構造圖5.6中的輸入數組`X`來驗證二維最大池化層的輸出。

``` python
X = torch.tensor([[0, 1, 2], [3, 4, 5], [6, 7, 8]])
pool2d(X, (2, 2))
```
輸出：
```
tensor([[4., 5.],
        [7., 8.]])
```

同時我們實驗一下平均池化層。

``` python
pool2d(X, (2, 2), 'avg')
```
輸出：
```
tensor([[2., 3.],
        [5., 6.]])
```

## 5.4.2 填充和步幅

同卷積層一樣，池化層也可以在輸入的高和寬兩側的填充並調整窗口的移動步幅來改變輸出形狀。池化層填充和步幅與卷積層填充和步幅的工作機制一樣。我們將通過`nn`模塊裡的二維最大池化層`MaxPool2d`來演示池化層填充和步幅的工作機制。我們先構造一個形狀為(1, 1, 4, 4)的輸入數據，前兩個維度分別是批量和通道。

``` python
X = torch.arange(16, dtype=torch.float).view((1, 1, 4, 4))
X
```
輸出：
```
tensor([[[[ 0.,  1.,  2.,  3.],
          [ 4.,  5.,  6.,  7.],
          [ 8.,  9., 10., 11.],
          [12., 13., 14., 15.]]]])
```

默認情況下，`MaxPool2d`實例裡步幅和池化窗口形狀相同。下面使用形狀為(3, 3)的池化窗口，默認獲得形狀為(3, 3)的步幅。

``` python
pool2d = nn.MaxPool2d(3)
pool2d(X) 
```
輸出：
```
tensor([[[[10.]]]])
```

我們可以手動指定步幅和填充。

``` python
pool2d = nn.MaxPool2d(3, padding=1, stride=2)
pool2d(X)
```
輸出：
```
tensor([[[[ 5.,  7.],
          [13., 15.]]]])
```

當然，我們也可以指定非正方形的池化窗口，並分別指定高和寬上的填充和步幅。

``` python
pool2d = nn.MaxPool2d((2, 4), padding=(1, 2), stride=(2, 3))
pool2d(X)
```
輸出：
```
tensor([[[[ 1.,  3.],
          [ 9., 11.],
          [13., 15.]]]])
```

## 5.4.3 多通道

在處理多通道輸入數據時，**池化層對每個輸入通道分別池化，而不是像卷積層那樣將各通道的輸入按通道相加**。這意味著池化層的輸出通道數與輸入通道數相等。下面將數組`X`和`X+1`在通道維上連結來構造通道數為2的輸入。

``` python
X = torch.cat((X, X + 1), dim=1)
X
```
輸出：
```
tensor([[[[ 0.,  1.,  2.,  3.],
          [ 4.,  5.,  6.,  7.],
          [ 8.,  9., 10., 11.],
          [12., 13., 14., 15.]],

         [[ 1.,  2.,  3.,  4.],
          [ 5.,  6.,  7.,  8.],
          [ 9., 10., 11., 12.],
          [13., 14., 15., 16.]]]])
```

池化後，我們發現輸出通道數仍然是2。

``` python
pool2d = nn.MaxPool2d(3, padding=1, stride=2)
pool2d(X)
```
輸出：
```
tensor([[[[ 5.,  7.],
          [13., 15.]],

         [[ 6.,  8.],
          [14., 16.]]]])
```

## 小結

* 最大池化和平均池化分別取池化窗口中輸入元素的最大值和平均值作為輸出。
* 池化層的一個主要作用是緩解卷積層對位置的過度敏感性。
* 可以指定池化層的填充和步幅。
* 池化層的輸出通道數跟輸入通道數相同。


-----------
> 注：除代碼外本節與原書此節基本相同，[原書傳送門](https://zh.d2l.ai/chapter_convolutional-neural-networks/pooling.html)



