# 5.3 多輸入通道和多輸出通道

前面兩節裡我們用到的輸入和輸出都是二維數組，但真實數據的維度經常更高。例如，彩色圖像在高和寬2個維度外還有RGB（紅、綠、藍）3個顏色通道。假設彩色圖像的高和寬分別是$h$和$w$（像素），那麼它可以表示為一個$3\times h\times w$的多維數組。我們將大小為3的這一維稱為通道（channel）維。本節我們將介紹含多個輸入通道或多個輸出通道的卷積核。



## 5.3.1 多輸入通道

當輸入數據含多個通道時，我們需要構造一個輸入通道數與輸入數據的通道數相同的卷積核，從而能夠與含多通道的輸入數據做互相關運算。假設輸入數據的通道數為$c_i$，那麼卷積核的輸入通道數同樣為$c_i$。設卷積核窗口形狀為$k_h\times k_w$。當$c_i=1$時，我們知道卷積核只包含一個形狀為$k_h\times k_w$的二維數組。當$c_i > 1$時，我們將會為每個輸入通道各分配一個形狀為$k_h\times k_w$的核數組。把這$c_i$個數組在輸入通道維上連結，即得到一個形狀為$c_i\times k_h\times k_w$的卷積核。由於輸入和卷積核各有$c_i$個通道，我們可以在各個通道上對輸入的二維數組和卷積核的二維核數組做互相關運算，再將這$c_i$個互相關運算的二維輸出按通道相加，得到一個二維數組。這就是含多個通道的輸入數據與多輸入通道的卷積核做二維互相關運算的輸出。

圖5.4展示了含2個輸入通道的二維互相關計算的例子。在每個通道上，二維輸入數組與二維核數組做互相關運算，再按通道相加即得到輸出。圖5.4中陰影部分為第一個輸出元素及其計算所使用的輸入和核數組元素：$(1\times1+2\times2+4\times3+5\times4)+(0\times0+1\times1+3\times2+4\times3)=56$。

<div align=center>
<img width="400" src="../img/chapter05/5.3_conv_multi_in.svg"/>
</div>
<div align=center>圖5.4 含2個輸入通道的互相關計算</div>


接下來我們實現含多個輸入通道的互相關運算。我們只需要對每個通道做互相關運算，然後通過`add_n`函數來進行累加。

``` python
import torch
from torch import nn
import sys
sys.path.append("..") 
import d2lzh_pytorch as d2l

def corr2d_multi_in(X, K):
    # 沿著X和K的第0維（通道維）分別計算再相加
    res = d2l.corr2d(X[0, :, :], K[0, :, :])
    for i in range(1, X.shape[0]):
        res += d2l.corr2d(X[i, :, :], K[i, :, :])
    return res
```

我們可以構造圖5.4中的輸入數組`X`、核數組`K`來驗證互相關運算的輸出。

``` python
X = torch.tensor([[[0, 1, 2], [3, 4, 5], [6, 7, 8]],
              [[1, 2, 3], [4, 5, 6], [7, 8, 9]]])
K = torch.tensor([[[0, 1], [2, 3]], [[1, 2], [3, 4]]])

corr2d_multi_in(X, K)
```
輸出：
```
tensor([[ 56.,  72.],
        [104., 120.]])
```

## 5.3.2 多輸出通道

當輸入通道有多個時，因為我們對各個通道的結果做了累加，所以不論輸入通道數是多少，輸出通道數總是為1。設卷積核輸入通道數和輸出通道數分別為$c_i$和$c_o$，高和寬分別為$k_h$和$k_w$。如果希望得到含多個通道的輸出，我們可以為每個輸出通道分別創建形狀為$c_i\times k_h\times k_w$的核數組。將它們在輸出通道維上連結，卷積核的形狀即$c_o\times c_i\times k_h\times k_w$。在做互相關運算時，每個輸出通道上的結果由卷積核在該輸出通道上的核數組與整個輸入數組計算而來。

下面我們實現一個互相關運算函數來計算多個通道的輸出。

``` python
def corr2d_multi_in_out(X, K):
    # 對K的第0維遍歷，每次同輸入X做互相關計算。所有結果使用stack函數合併在一起
    return torch.stack([corr2d_multi_in(X, k) for k in K])
```

我們將核數組`K`同`K+1`（`K`中每個元素加一）和`K+2`連結在一起來構造一個輸出通道數為3的卷積核。

``` python
K = torch.stack([K, K + 1, K + 2])
K.shape # torch.Size([3, 2, 2, 2])
```

下面我們對輸入數組`X`與核數組`K`做互相關運算。此時的輸出含有3個通道。其中第一個通道的結果與之前輸入數組`X`與多輸入通道、單輸出通道核的計算結果一致。

```python
corr2d_multi_in_out(X, K)
```
輸出：
```
tensor([[[ 56.,  72.],
         [104., 120.]],

        [[ 76., 100.],
         [148., 172.]],

        [[ 96., 128.],
         [192., 224.]]])
```


## 5.3.3 $1\times 1$卷積層

最後我們討論卷積窗口形狀為$1\times 1$（$k_h=k_w=1$）的多通道卷積層。我們通常稱之為$1\times 1$卷積層，並將其中的卷積運算稱為$1\times 1$卷積。因為使用了最小窗口，$1\times 1$卷積失去了卷積層可以識別高和寬維度上相鄰元素構成的模式的功能。實際上，$1\times 1$卷積的主要計算髮生在通道維上。圖5.5展示了使用輸入通道數為3、輸出通道數為2的$1\times 1$卷積核的互相關計算。值得注意的是，輸入和輸出具有相同的高和寬。輸出中的每個元素來自輸入中在高和寬上相同位置的元素在不同通道之間的按權重累加。假設我們將通道維當作特徵維，將高和寬維度上的元素當成數據樣本，**那麼$1\times 1$卷積層的作用與全連接層等價**。

<div align=center>
<img width="400" src="../img/chapter05/5.3_conv_1x1.svg"/>
</div>
<div align=center>圖5.5 1x1卷積核的互相關計算。輸入和輸出具有相同的高和寬</div>


下面我們使用全連接層中的矩陣乘法來實現$1\times 1$卷積。這裡需要在矩陣乘法運算前後對數據形狀做一些調整。

``` python
def corr2d_multi_in_out_1x1(X, K):
    c_i, h, w = X.shape
    c_o = K.shape[0]
    X = X.view(c_i, h * w)
    K = K.view(c_o, c_i)
    Y = torch.mm(K, X)  # 全連接層的矩陣乘法
    return Y.view(c_o, h, w)
```

經驗證，做$1\times 1$卷積時，以上函數與之前實現的互相關運算函數`corr2d_multi_in_out`等價。

``` python
X = torch.rand(3, 3, 3)
K = torch.rand(2, 3, 1, 1)

Y1 = corr2d_multi_in_out_1x1(X, K)
Y2 = corr2d_multi_in_out(X, K)

(Y1 - Y2).norm().item() < 1e-6
```
輸出：
```
True
```

在之後的模型裡我們將會看到$1\times 1$卷積層被當作保持高和寬維度形狀不變的全連接層使用。於是，我們可以通過調整網絡層之間的通道數來控制模型複雜度。


## 小結

* 使用多通道可以拓展卷積層的模型參數。
* 假設將通道維當作特徵維，將高和寬維度上的元素當成數據樣本，那麼$1\times 1$卷積層的作用與全連接層等價。
* $1\times 1$卷積層通常用來調整網絡層之間的通道數，並控制模型複雜度。


-----------
> 注：除代碼外本節與原書此節基本相同，[原書傳送門](https://zh.d2l.ai/chapter_convolutional-neural-networks/channels.html)
