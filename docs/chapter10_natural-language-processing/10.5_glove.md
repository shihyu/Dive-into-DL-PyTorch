# 10.5 全局向量的詞嵌入（GloVe）

讓我們先回顧一下word2vec中的跳字模型。將跳字模型中使用softmax運算表達的條件概率$P(w_j\mid w_i)$記作$q_{ij}$，即

$$
q_{ij}=\frac{\exp(\boldsymbol{u}_j^\top \boldsymbol{v}_i)}{ \sum_{k \in \mathcal{V}} \text{exp}(\boldsymbol{u}_k^\top \boldsymbol{v}_i)},
$$

其中$\boldsymbol{v}_i$和$\boldsymbol{u}_i$分別是索引為$i$的詞$w_i$作為中心詞和背景詞時的向量表示，$\mathcal{V} = \{0, 1, \ldots, |\mathcal{V}|-1\}$為詞典索引集。

對於詞$w_i$，它在數據集中可能多次出現。我們將每一次以它作為中心詞的所有背景詞全部彙總並保留重複元素，記作多重集（multiset）$\mathcal{C}_i$。一個元素在多重集中的個數稱為該元素的重數（multiplicity）。舉例來說，假設詞$w_i$在數據集中出現2次：文本序列中以這2個$w_i$作為中心詞的背景窗口分別包含背景詞索引$2,1,5,2$和$2,3,2,1$。那麼多重集$\mathcal{C}_i = \{1,1,2,2,2,2,3,5\}$，其中元素1的重數為2，元素2的重數為4，元素3和5的重數均為1。將多重集$\mathcal{C}_i$中元素$j$的重數記作$x_{ij}$：它表示了整個數據集中所有以$w_i$為中心詞的背景窗口中詞$w_j$的個數。那麼，跳字模型的損失函數還可以用另一種方式表達：

$$
-\sum_{i\in\mathcal{V}}\sum_{j\in\mathcal{V}} x_{ij} \log\,q_{ij}.
$$

我們將數據集中所有以詞$w_i$為中心詞的背景詞的數量之和$\left|\mathcal{C}_i\right|$記為$x_i$，並將以$w_i$為中心詞生成背景詞$w_j$的條件概率$x_{ij}/x_i$記作$p_{ij}$。我們可以進一步改寫跳字模型的損失函數為

$$
-\sum_{i\in\mathcal{V}} x_i \sum_{j\in\mathcal{V}} p_{ij} \log\,q_{ij}.
$$

上式中，$-\sum_{j\in\mathcal{V}} p_{ij} \log\,q_{ij}$計算的是以$w_i$為中心詞的背景詞條件概率分佈$p_{ij}$和模型預測的條件概率分佈$q_{ij}$的交叉熵，且損失函數使用所有以詞$w_i$為中心詞的背景詞的數量之和來加權。最小化上式中的損失函數會令預測的條件概率分佈儘可能接近真實的條件概率分佈。

然而，作為常用損失函數的一種，交叉熵損失函數有時並不是好的選擇。一方面，正如我們在10.2節（近似訓練）中所提到的，令模型預測$q_{ij}$成為合法概率分佈的代價是它在分母中基於整個詞典的累加項。這很容易帶來過大的計算開銷。另一方面，詞典中往往有大量生僻詞，它們在數據集中出現的次數極少。而有關大量生僻詞的條件概率分佈在交叉熵損失函數中的最終預測往往並不準確。



## 10.5.1 GloVe模型

鑑於此，作為在word2vec之後提出的詞嵌入模型，GloVe模型採用了平方損失，並基於該損失對跳字模型做了3點改動 [1]：

1. 使用非概率分佈的變量$p'_{ij}=x_{ij}$和$q'_{ij}=\exp(\boldsymbol{u}_j^\top \boldsymbol{v}_i)$，並對它們取對數。因此，平方損失項是$\left(\log\,p'_{ij} - \log\,q'_{ij}\right)^2 = \left(\boldsymbol{u}_j^\top \boldsymbol{v}_i - \log\,x_{ij}\right)^2$。
2. 為每個詞$w_i$增加兩個為標量的模型參數：中心詞偏差項$b_i$和背景詞偏差項$c_i$。
3. 將每個損失項的權重替換成函數$h(x_{ij})$。權重函數$h(x)$是值域在$[0,1]$的單調遞增函數。

如此一來，GloVe模型的目標是最小化損失函數

$$\sum_{i\in\mathcal{V}} \sum_{j\in\mathcal{V}} h(x_{ij}) \left(\boldsymbol{u}_j^\top \boldsymbol{v}_i + b_i + c_j - \log\,x_{ij}\right)^2.$$

其中權重函數$h(x)$的一個建議選擇是：當$x < c$時（如$c = 100$），令$h(x) = (x/c)^\alpha$（如$\alpha = 0.75$），反之令$h(x) = 1$。因為$h(0)=0$，所以對於$x_{ij}=0$的平方損失項可以直接忽略。當使用小批量隨機梯度下降來訓練時，每個時間步我們隨機採樣小批量非零$x_{ij}$，然後計算梯度來迭代模型參數。這些非零$x_{ij}$是預先基於整個數據集計算得到的，包含了數據集的全局統計信息。因此，GloVe模型的命名取“全局向量”（Global Vectors）之意。

需要強調的是，如果詞$w_i$出現在詞$w_j$的背景窗口裡，那麼詞$w_j$也會出現在詞$w_i$的背景窗口裡。也就是說，$x_{ij}=x_{ji}$。不同於word2vec中擬合的是非對稱的條件概率$p_{ij}$，GloVe模型擬合的是對稱的$\log\, x_{ij}$。因此，任意詞的中心詞向量和背景詞向量在GloVe模型中是等價的。但由於初始化值的不同，同一個詞最終學習到的兩組詞向量可能不同。當學習得到所有詞向量以後，GloVe模型使用中心詞向量與背景詞向量之和作為該詞的最終詞向量。


## 10.5.2 從條件概率比值理解GloVe模型

我們還可以從另外一個角度來理解GloVe模型。沿用本節前面的符號，$P(w_j \mid w_i)$表示數據集中以$w_i$為中心詞生成背景詞$w_j$的條件概率，並記作$p_{ij}$。作為源於某大型語料庫的真實例子，以下列舉了兩組分別以“ice”（冰）和“steam”（蒸汽）為中心詞的條件概率以及它們之間的比值 [1]：

|$w_k$=|“solid”|“gas”|“water”|“fashion”|
|--:|:-:|:-:|:-:|:-:|
|$p_1=P(w_k\mid$ “ice” $)$|0.00019|0.000066|0.003|0.000017|
|$p_2=P(w_k\mid$ “steam” $)$|0.000022|0.00078|0.0022|0.000018|
|$p_1/p_2$|8.9|0.085|1.36|0.96|


我們可以觀察到以下現象。

* 對於與“ice”相關而與“steam”不相關的詞$w_k$，如$w_k=$“solid”（固體），我們期望條件概率比值較大，如上表最後一行中的值8.9；
* 對於與“ice”不相關而與“steam”相關的詞$w_k$，如$w_k=$“gas”（氣體），我們期望條件概率比值較小，如上表最後一行中的值0.085；
* 對於與“ice”和“steam”都相關的詞$w_k$，如$w_k=$“water”（水），我們期望條件概率比值接近1，如上表最後一行中的值1.36；
* 對於與“ice”和“steam”都不相關的詞$w_k$，如$w_k=$“fashion”（時尚），我們期望條件概率比值接近1，如上表最後一行中的值0.96。

由此可見，條件概率比值能比較直觀地表達詞與詞之間的關係。我們可以構造一個詞向量函數使它能有效擬合條件概率比值。我們知道，任意一個這樣的比值需要3個詞$w_i$、$w_j$和$w_k$。以$w_i$作為中心詞的條件概率比值為${p_{ij}}/{p_{ik}}$。我們可以找一個函數，它使用詞向量來擬合這個條件概率比值

$$
f(\boldsymbol{u}_j, \boldsymbol{u}_k, {\boldsymbol{v}}_i) \approx \frac{p_{ij}}{p_{ik}}.
$$

這裡函數$f$可能的設計並不唯一，我們只需考慮一種較為合理的可能性。注意到條件概率比值是一個標量，我們可以將$f$限制為一個標量函數：$f(\boldsymbol{u}_j, \boldsymbol{u}_k, {\boldsymbol{v}}_i) = f\left((\boldsymbol{u}_j - \boldsymbol{u}_k)^\top {\boldsymbol{v}}_i\right)$。交換索引$j$和$k$後可以看到函數$f$應該滿足$f(x)f(-x)=1$，因此一種可能是$f(x)=\exp(x)$，於是

$$f
(\boldsymbol{u}_j, \boldsymbol{u}_k, {\boldsymbol{v}}_i) = \frac{\exp\left(\boldsymbol{u}_j^\top {\boldsymbol{v}}_i\right)}{\exp\left(\boldsymbol{u}_k^\top {\boldsymbol{v}}_i\right)} \approx \frac{p_{ij}}{p_{ik}}.
$$

滿足最右邊約等號的一種可能是$\exp\left(\boldsymbol{u}_j^\top {\boldsymbol{v}}_i\right) \approx \alpha p_{ij}$，這裡$\alpha$是一個常數。考慮到$p_{ij}=x_{ij}/x_i$，取對數後$\boldsymbol{u}_j^\top {\boldsymbol{v}}_i \approx \log\,\alpha + \log\,x_{ij} - \log\,x_i$。我們使用額外的偏差項來擬合$- \log\,\alpha + \log\,x_i$，例如，中心詞偏差項$b_i$和背景詞偏差項$c_j$：

$$
\boldsymbol{u}_j^\top \boldsymbol{v}_i + b_i + c_j \approx \log(x_{ij}).
$$

對上式左右兩邊取平方誤差並加權，我們可以得到GloVe模型的損失函數。


## 小結

* 在有些情況下，交叉熵損失函數有劣勢。GloVe模型採用了平方損失，並通過詞向量擬合預先基於整個數據集計算得到的全局統計信息。
* 任意詞的中心詞向量和背景詞向量在GloVe模型中是等價的。



## 參考文獻

[1] Pennington, J., Socher, R., & Manning, C. (2014). Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP) (pp. 1532-1543).


-----------
> 注：本節與原書完全相同，[原書傳送門](https://zh.d2l.ai/chapter_natural-language-processing/glove.html)


