# 5.2 填充和步幅

在上一節的例子裡，我們使用高和寬為3的輸入與高和寬為2的卷積核得到高和寬為2的輸出。一般來說，假設輸入形狀是$n_h\times n_w$，卷積核窗口形狀是$k_h\times k_w$，那麼輸出形狀將會是

$$(n_h-k_h+1) \times (n_w-k_w+1).$$

所以卷積層的輸出形狀由輸入形狀和卷積核窗口形狀決定。本節我們將介紹卷積層的兩個超參數，即填充和步幅。它們可以對給定形狀的輸入和卷積核改變輸出形狀。

## 5.2.1 填充

填充（padding）是指在輸入高和寬的兩側填充元素（通常是0元素）。圖5.2裡我們在原輸入高和寬的兩側分別添加了值為0的元素，使得輸入高和寬從3變成了5，並導致輸出高和寬由2增加到4。圖5.2中的陰影部分為第一個輸出元素及其計算所使用的輸入和核數組元素：$0\times0+0\times1+0\times2+0\times3=0$。

<div align=center>
<img width="400" src="../img/chapter05/5.2_conv_pad.svg"/>
</div>
<div align=center>圖5.2 在輸入的高和寬兩側分別填充了0元素的二維互相關計算</div>

一般來說，如果在高的兩側一共填充$p_h$行，在寬的兩側一共填充$p_w$列，那麼輸出形狀將會是

$$(n_h-k_h+p_h+1)\times(n_w-k_w+p_w+1),$$

也就是說，輸出的高和寬會分別增加$p_h$和$p_w$。

在很多情況下，我們會設置$p_h=k_h-1$和$p_w=k_w-1$來使輸入和輸出具有相同的高和寬。這樣會方便在構造網絡時推測每個層的輸出形狀。假設這裡$k_h$是奇數，我們會在高的兩側分別填充$p_h/2$行。如果$k_h$是偶數，一種可能是在輸入的頂端一側填充$\lceil p_h/2\rceil$行，而在底端一側填充$\lfloor p_h/2\rfloor$行。在寬的兩側填充同理。

卷積神經網絡經常使用奇數高寬的卷積核，如1、3、5和7，所以兩端上的填充個數相等。對任意的二維數組`X`，設它的第`i`行第`j`列的元素為`X[i,j]`。當兩端上的填充個數相等，並使輸入和輸出具有相同的高和寬時，我們就知道輸出`Y[i,j]`是由輸入以`X[i,j]`為中心的窗口同卷積核進行互相關計算得到的。

下面的例子裡我們創建一個高和寬為3的二維卷積層，然後設輸入高和寬兩側的填充數分別為1。給定一個高和寬為8的輸入，我們發現輸出的高和寬也是8。

``` python
import torch
from torch import nn

# 定義一個函數來計算卷積層。它對輸入和輸出做相應的升維和降維
def comp_conv2d(conv2d, X):
    # (1, 1)代表批量大小和通道數（“多輸入通道和多輸出通道”一節將介紹）均為1
    X = X.view((1, 1) + X.shape)
    Y = conv2d(X)
    return Y.view(Y.shape[2:])  # 排除不關心的前兩維：批量和通道

# 注意這裡是兩側分別填充1行或列，所以在兩側一共填充2行或列
conv2d = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, padding=1)

X = torch.rand(8, 8)
comp_conv2d(conv2d, X).shape
```
輸出：
```
torch.Size([8, 8])
```

當卷積核的高和寬不同時，我們也可以通過設置高和寬上不同的填充數使輸出和輸入具有相同的高和寬。

``` python
# 使用高為5、寬為3的卷積核。在高和寬兩側的填充數分別為2和1
conv2d = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(5, 3), padding=(2, 1))
comp_conv2d(conv2d, X).shape
```
輸出：
```
torch.Size([8, 8])
```

## 5.2.2 步幅

在上一節裡我們介紹了二維互相關運算。卷積窗口從輸入數組的最左上方開始，按從左往右、從上往下的順序，依次在輸入數組上滑動。我們將每次滑動的行數和列數稱為步幅（stride）。

目前我們看到的例子裡，在高和寬兩個方向上步幅均為1。我們也可以使用更大步幅。圖5.3展示了在高上步幅為3、在寬上步幅為2的二維互相關運算。可以看到，輸出第一列第二個元素時，卷積窗口向下滑動了3行，而在輸出第一行第二個元素時卷積窗口向右滑動了2列。當卷積窗口在輸入上再向右滑動2列時，由於輸入元素無法填滿窗口，無結果輸出。圖5.3中的陰影部分為輸出元素及其計算所使用的輸入和核數組元素：$0\times0+0\times1+1\times2+2\times3=8$、$0\times0+6\times1+0\times2+0\times3=6$。

<div align=center>
<img width="400" src="../img/chapter05/5.2_conv_stride.svg"/>
</div>
<div align=center>圖5.3 高和寬上步幅分別為3和2的二維互相關運算</div>

一般來說，當高上步幅為$s_h$，寬上步幅為$s_w$時，輸出形狀為

$$\lfloor(n_h-k_h+p_h+s_h)/s_h\rfloor \times \lfloor(n_w-k_w+p_w+s_w)/s_w\rfloor.$$

如果設置$p_h=k_h-1$和$p_w=k_w-1$，那麼輸出形狀將簡化為$\lfloor(n_h+s_h-1)/s_h\rfloor \times \lfloor(n_w+s_w-1)/s_w\rfloor$。更進一步，如果輸入的高和寬能分別被高和寬上的步幅整除，那麼輸出形狀將是$(n_h/s_h) \times (n_w/s_w)$。

下面我們令高和寬上的步幅均為2，從而使輸入的高和寬減半。

``` python
conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1, stride=2)
comp_conv2d(conv2d, X).shape
```
輸出：
```
torch.Size([4, 4])
```

接下來是一個稍微複雜點兒的例子。

``` python
conv2d = nn.Conv2d(1, 1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))
comp_conv2d(conv2d, X).shape
```
輸出：
```
torch.Size([2, 2])
```

為了表述簡潔，當輸入的高和寬兩側的填充數分別為$p_h$和$p_w$時，我們稱填充為$(p_h, p_w)$。特別地，當$p_h = p_w = p$時，填充為$p$。當在高和寬上的步幅分別為$s_h$和$s_w$時，我們稱步幅為$(s_h, s_w)$。特別地，當$s_h = s_w = s$時，步幅為$s$。在默認情況下，填充為0，步幅為1。



## 小結

* 填充可以增加輸出的高和寬。這常用來使輸出與輸入具有相同的高和寬。
* 步幅可以減小輸出的高和寬，例如輸出的高和寬僅為輸入的高和寬的$1/n$（$n$為大於1的整數）。

-----------
> 注：除代碼外本節與原書此節基本相同，[原書傳送門](https://zh.d2l.ai/chapter_convolutional-neural-networks/padding-and-strides.html)


