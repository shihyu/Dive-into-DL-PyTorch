# 6.1 語言模型

語言模型（language model）是自然語言處理的重要技術。自然語言處理中最常見的數據是文本數據。我們可以把一段自然語言文本看作一段離散的時間序列。假設一段長度為$T$的文本中的詞依次為$w_1, w_2, \ldots, w_T$，那麼在離散的時間序列中，$w_t$（$1 \leq t \leq T$）可看作在時間步（time step）$t$的輸出或標籤。給定一個長度為$T$的詞的序列$w_1, w_2, \ldots, w_T$，語言模型將計算該序列的概率：

$$P(w_1, w_2, \ldots, w_T).$$


語言模型可用於提升語音識別和機器翻譯的性能。例如，在語音識別中，給定一段“廚房裡食油用完了”的語音，有可能會輸出“廚房裡食油用完了”和“廚房裡石油用完了”這兩個讀音完全一樣的文本序列。如果語言模型判斷出前者的概率大於後者的概率，我們就可以根據相同讀音的語音輸出“廚房裡食油用完了”的文本序列。在機器翻譯中，如果對英文“you go first”逐詞翻譯成中文的話，可能得到“你走先”“你先走”等排列方式的文本序列。如果語言模型判斷出“你先走”的概率大於其他排列方式的文本序列的概率，我們就可以把“you go first”翻譯成“你先走”。


## 6.1.1 語言模型的計算


既然語言模型很有用，那該如何計算它呢？假設序列$w_1, w_2, \ldots, w_T$中的每個詞是依次生成的，我們有

$$P(w_1, w_2, \ldots, w_T) = \prod_{t=1}^T P(w_t \mid w_1, \ldots, w_{t-1}).$$

例如，一段含有4個詞的文本序列的概率

$$P(w_1, w_2, w_3, w_4) =  P(w_1) P(w_2 \mid w_1) P(w_3 \mid w_1, w_2) P(w_4 \mid w_1, w_2, w_3).$$

為了計算語言模型，我們需要計算詞的概率，以及一個詞在給定前幾個詞的情況下的條件概率，即語言模型參數。設訓練數據集為一個大型文本語料庫，如維基百科的所有條目。詞的概率可以通過該詞在訓練數據集中的相對詞頻來計算。例如，$P(w_1)$可以計算為$w_1$在訓練數據集中的詞頻（詞出現的次數）與訓練數據集的總詞數之比。因此，根據條件概率定義，一個詞在給定前幾個詞的情況下的條件概率也可以通過訓練數據集中的相對詞頻計算。例如，$P(w_2 \mid w_1)$可以計算為$w_1, w_2$兩詞相鄰的頻率與$w_1$詞頻的比值，因為該比值即$P(w_1, w_2)$與$P(w_1)$之比；而$P(w_3 \mid w_1, w_2)$同理可以計算為$w_1$、$w_2$和$w_3$三詞相鄰的頻率與$w_1$和$w_2$兩詞相鄰的頻率的比值。以此類推。


## 6.1.2 $n$元語法

當序列長度增加時，計算和存儲多個詞共同出現的概率的複雜度會呈指數級增加。$n$元語法通過馬爾可夫假設（雖然並不一定成立）簡化了語言模型的計算。這裡的馬爾可夫假設是指一個詞的出現只與前面$n$個詞相關，即$n$階馬爾可夫鏈（Markov chain of order $n$）。如果$n=1$，那麼有$P(w_3 \mid w_1, w_2) = P(w_3 \mid w_2)$。如果基於$n-1$階馬爾可夫鏈，我們可以將語言模型改寫為

$$P(w_1, w_2, \ldots, w_T) \approx \prod_{t=1}^T P(w_t \mid w_{t-(n-1)}, \ldots, w_{t-1}) .$$


以上也叫$n$元語法（$n$-grams）。它是基於$n - 1$階馬爾可夫鏈的概率語言模型。當$n$分別為1、2和3時，我們將其分別稱作一元語法（unigram）、二元語法（bigram）和三元語法（trigram）。例如，長度為4的序列$w_1, w_2, w_3, w_4$在一元語法、二元語法和三元語法中的概率分別為

$$
\begin{aligned}
P(w_1, w_2, w_3, w_4) &=  P(w_1) P(w_2) P(w_3) P(w_4) ,\\
P(w_1, w_2, w_3, w_4) &=  P(w_1) P(w_2 \mid w_1) P(w_3 \mid w_2) P(w_4 \mid w_3) ,\\
P(w_1, w_2, w_3, w_4) &=  P(w_1) P(w_2 \mid w_1) P(w_3 \mid w_1, w_2) P(w_4 \mid w_2, w_3) .
\end{aligned}
$$

當$n$較小時，$n$元語法往往並不準確。例如，在一元語法中，由三個詞組成的句子“你走先”和“你先走”的概率是一樣的。然而，當$n$較大時，$n$元語法需要計算並存儲大量的詞頻和多詞相鄰頻率。

那麼，有沒有方法在語言模型中更好地平衡以上這兩點呢？我們將在本章探究這樣的方法。

## 小結

* 語言模型是自然語言處理的重要技術。
* $N$元語法是基於$n-1$階馬爾可夫鏈的概率語言模型，其中$n$權衡了計算複雜度和模型準確性。

-----------
> 注：本節與原書此節完全相同，[原書傳送門](https://zh.d2l.ai/chapter_recurrent-neural-networks/lang-model.html)