# 4.4 自定義層

深度學習的一個魅力在於神經網絡中各式各樣的層，例如全連接層和後面章節中將要介紹的卷積層、池化層與循環層。雖然PyTorch提供了大量常用的層，但有時候我們依然希望自定義層。本節將介紹如何使用`Module`來自定義層，從而可以被重複調用。


## 4.4.1 不含模型參數的自定義層

我們先介紹如何定義一個不含模型參數的自定義層。事實上，這和4.1節（模型構造）中介紹的使用`Module`類構造模型類似。下面的`CenteredLayer`類通過繼承`Module`類自定義了一個將輸入減掉均值後輸出的層，並將層的計算定義在了`forward`函數裡。這個層裡不含模型參數。

``` python
import torch
from torch import nn

class CenteredLayer(nn.Module):
    def __init__(self, **kwargs):
        super(CenteredLayer, self).__init__(**kwargs)
    def forward(self, x):
        return x - x.mean()
```

我們可以實例化這個層，然後做前向計算。

``` python
layer = CenteredLayer()
layer(torch.tensor([1, 2, 3, 4, 5], dtype=torch.float))
```
輸出：
```
tensor([-2., -1.,  0.,  1.,  2.])
```

我們也可以用它來構造更復雜的模型。

``` python
net = nn.Sequential(nn.Linear(8, 128), CenteredLayer())
```

下面打印自定義層各個輸出的均值。因為均值是浮點數，所以它的值是一個很接近0的數。

``` python
y = net(torch.rand(4, 8))
y.mean().item()
```
輸出：
```
0.0
```

## 4.4.2 含模型參數的自定義層

我們還可以自定義含模型參數的自定義層。其中的模型參數可以通過訓練學出。

在4.2節（模型參數的訪問、初始化和共享）中介紹了`Parameter`類其實是`Tensor`的子類，如果一個`Tensor`是`Parameter`，那麼它會自動被添加到模型的參數列表裡。所以在自定義含模型參數的層時，我們應該將參數定義成`Parameter`，除了像4.2.1節那樣直接定義成`Parameter`類外，還可以使用`ParameterList`和`ParameterDict`分別定義參數的列表和字典。

`ParameterList`接收一個`Parameter`實例的列表作為輸入然後得到一個參數列表，使用的時候可以用索引來訪問某個參數，另外也可以使用`append`和`extend`在列表後面新增參數。
``` python
class MyDense(nn.Module):
    def __init__(self):
        super(MyDense, self).__init__()
        self.params = nn.ParameterList([nn.Parameter(torch.randn(4, 4)) for i in range(3)])
        self.params.append(nn.Parameter(torch.randn(4, 1)))

    def forward(self, x):
        for i in range(len(self.params)):
            x = torch.mm(x, self.params[i])
        return x
net = MyDense()
print(net)
```
輸出：
```
MyDense(
  (params): ParameterList(
      (0): Parameter containing: [torch.FloatTensor of size 4x4]
      (1): Parameter containing: [torch.FloatTensor of size 4x4]
      (2): Parameter containing: [torch.FloatTensor of size 4x4]
      (3): Parameter containing: [torch.FloatTensor of size 4x1]
  )
)
```
而`ParameterDict`接收一個`Parameter`實例的字典作為輸入然後得到一個參數字典，然後可以按照字典的規則使用了。例如使用`update()`新增參數，使用`keys()`返回所有鍵值，使用`items()`返回所有鍵值對等等，可參考[官方文檔](https://pytorch.org/docs/stable/nn.html#parameterdict)。

``` python
class MyDictDense(nn.Module):
    def __init__(self):
        super(MyDictDense, self).__init__()
        self.params = nn.ParameterDict({
                'linear1': nn.Parameter(torch.randn(4, 4)),
                'linear2': nn.Parameter(torch.randn(4, 1))
        })
        self.params.update({'linear3': nn.Parameter(torch.randn(4, 2))}) # 新增

    def forward(self, x, choice='linear1'):
        return torch.mm(x, self.params[choice])

net = MyDictDense()
print(net)
```
輸出：
```
MyDictDense(
  (params): ParameterDict(
      (linear1): Parameter containing: [torch.FloatTensor of size 4x4]
      (linear2): Parameter containing: [torch.FloatTensor of size 4x1]
      (linear3): Parameter containing: [torch.FloatTensor of size 4x2]
  )
)
```
這樣就可以根據傳入的鍵值來進行不同的前向傳播：
``` python
x = torch.ones(1, 4)
print(net(x, 'linear1'))
print(net(x, 'linear2'))
print(net(x, 'linear3'))
```
輸出：
```
tensor([[1.5082, 1.5574, 2.1651, 1.2409]], grad_fn=<MmBackward>)
tensor([[-0.8783]], grad_fn=<MmBackward>)
tensor([[ 2.2193, -1.6539]], grad_fn=<MmBackward>)
```

我們也可以使用自定義層構造模型。它和PyTorch的其他層在使用上很類似。

``` python
net = nn.Sequential(
    MyDictDense(),
    MyListDense(),
)
print(net)
print(net(x))
```
輸出：
```
Sequential(
  (0): MyDictDense(
    (params): ParameterDict(
        (linear1): Parameter containing: [torch.FloatTensor of size 4x4]
        (linear2): Parameter containing: [torch.FloatTensor of size 4x1]
        (linear3): Parameter containing: [torch.FloatTensor of size 4x2]
    )
  )
  (1): MyListDense(
    (params): ParameterList(
        (0): Parameter containing: [torch.FloatTensor of size 4x4]
        (1): Parameter containing: [torch.FloatTensor of size 4x4]
        (2): Parameter containing: [torch.FloatTensor of size 4x4]
        (3): Parameter containing: [torch.FloatTensor of size 4x1]
    )
  )
)
tensor([[-101.2394]], grad_fn=<MmBackward>)
```

## 小結

* 可以通過`Module`類自定義神經網絡中的層，從而可以被重複調用。


-----------
> 注：本節與原書此節有一些不同，[原書傳送門](https://zh.d2l.ai/chapter_deep-learning-computation/custom-layer.html)

