# 3.15 數值穩定性和模型初始化

理解了正向傳播與反向傳播以後，我們來討論一下深度學習模型的數值穩定性問題以及模型參數的初始化方法。深度模型有關數值穩定性的典型問題是衰減（vanishing）和爆炸（explosion）。


## 3.15.1 衰減和爆炸

當神經網絡的層數較多時，模型的數值穩定性容易變差。假設一個層數為$L$的多層感知機的第$l$層$\boldsymbol{H}^{(l)}$的權重參數為$\boldsymbol{W}^{(l)}$，輸出層$\boldsymbol{H}^{(L)}$的權重參數為$\boldsymbol{W}^{(L)}$。為了便於討論，不考慮偏差參數，且設所有隱藏層的激活函數為恆等映射（identity mapping）$\phi(x) = x$。給定輸入$\boldsymbol{X}$，多層感知機的第$l$層的輸出$\boldsymbol{H}^{(l)} = \boldsymbol{X} \boldsymbol{W}^{(1)} \boldsymbol{W}^{(2)} \ldots \boldsymbol{W}^{(l)}$。此時，如果層數$l$較大，$\boldsymbol{H}^{(l)}$的計算可能會出現衰減或爆炸。舉個例子，假設輸入和所有層的權重參數都是標量，如權重參數為0.2和5，多層感知機的第30層輸出為輸入$\boldsymbol{X}$分別與$0.2^{30} \approx 1 \times 10^{-21}$（衰減）和$5^{30} \approx 9 \times 10^{20}$（爆炸）的乘積。類似地，當層數較多時，梯度的計算也更容易出現衰減或爆炸。

隨著內容的不斷深入，我們會在後面的章節進一步介紹深度學習的數值穩定性問題以及解決方法。


## 3.15.2 隨機初始化模型參數

在神經網絡中，通常需要隨機初始化模型參數。下面我們來解釋這樣做的原因。

回顧3.8節（多層感知機）圖3.3描述的多層感知機。為了方便解釋，假設輸出層只保留一個輸出單元$o_1$（刪去$o_2$和$o_3$以及指向它們的箭頭），且隱藏層使用相同的激活函數。如果將每個隱藏單元的參數都初始化為相等的值，那麼在正向傳播時每個隱藏單元將根據相同的輸入計算出相同的值，並傳遞至輸出層。在反向傳播中，每個隱藏單元的參數梯度值相等。因此，這些參數在使用基於梯度的優化算法迭代後值依然相等。之後的迭代也是如此。在這種情況下，無論隱藏單元有多少，隱藏層本質上只有1個隱藏單元在發揮作用。因此，正如在前面的實驗中所做的那樣，我們通常將神經網絡的模型參數，特別是權重參數，進行隨機初始化。


### 3.15.2.1 PyTorch的默認隨機初始化

隨機初始化模型參數的方法有很多。在3.3節（線性迴歸的簡潔實現）中，我們使用`torch.nn.init.normal_()`使模型`net`的權重參數採用正態分佈的隨機初始化方式。不過，PyTorch中`nn.Module`的模塊參數都採取了較為合理的初始化策略（不同類型的layer具體採樣的哪一種初始化方法的可參考[源代碼](https://github.com/pytorch/pytorch/tree/master/torch/nn/modules)），因此一般不用我們考慮。


### 3.15.2.2 Xavier隨機初始化

還有一種比較常用的隨機初始化方法叫作Xavier隨機初始化[1]。
假設某全連接層的輸入個數為$a$，輸出個數為$b$，Xavier隨機初始化將使該層中權重參數的每個元素都隨機採樣於均勻分佈

$$U\left(-\sqrt{\frac{6}{a+b}}, \sqrt{\frac{6}{a+b}}\right).$$

它的設計主要考慮到，模型參數初始化後，每層輸出的方差不該受該層輸入個數影響，且每層梯度的方差也不該受該層輸出個數影響。

## 小結

* 深度模型有關數值穩定性的典型問題是衰減和爆炸。當神經網絡的層數較多時，模型的數值穩定性容易變差。
* 我們通常需要隨機初始化神經網絡的模型參數，如權重參數。


## 參考文獻

[1] Glorot, X., & Bengio, Y. (2010, March). Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics (pp. 249-256).